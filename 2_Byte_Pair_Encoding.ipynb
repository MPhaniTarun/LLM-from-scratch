{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "885214e8-d967-4757-bcc5-3d54902b1241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6916605a-1c7a-449a-88d4-f96c2bb2cf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba33d7-537d-46b7-aed7-3d8093418e47",
   "metadata": {},
   "source": [
    "<h2>GPT-2 Tokenizer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9ff477-30a3-450a-9694-21e19c79f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08e97f5d-1ea0-4107-aaf4-99ad5726e4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 28265, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 44, 2725, 3216, 47079, 403, 318, 616, 1336, 1438, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello!, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"MPhaniTarun is my full name.\")\n",
    "\n",
    "ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03b6cf67-8d0c-4f51-b9e0-2d97c7aeaec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!, do you like tea? <|endoftext|> In the sunlit terracesMPhaniTarun is my full name.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = tokenizer.decode(ids)\n",
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a7e5a9-b7b2-4eff-80fd-f777602e974b",
   "metadata": {},
   "source": [
    "<h2>GPT-3.5</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75011c1f-c70b-4ca5-b8f3-ea42e2a47b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17a8eb1a-e570-4c34-b21f-c284212f6d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9906, 17581, 656, 499, 1093, 15600, 30, 220, 100257, 763, 279, 7160, 32735, 7317, 2492, 44, 3438, 5676, 63833, 359, 374, 856, 2539, 836, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello!, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"MPhaniTarun is my full name.\")\n",
    "\n",
    "ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee63571c-c83e-4519-9388-d649c43ae6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!, do you like tea? <|endoftext|> In the sunlit terracesMPhaniTarun is my full name.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = tokenizer.decode(ids)\n",
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b180fa33-a936-4c91-a5d9-0c845c18dadf",
   "metadata": {},
   "source": [
    "<h2>GPT-4o</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b7bd91d-e6f0-45ba-b45f-0fc883ae7ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25b03e05-2210-4bc1-96e0-f5cc13b215db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13225, 27942, 621, 481, 1299, 17966, 30, 220, 199999, 730, 290, 7334, 32758, 173297, 44, 3780, 3048, 48287, 373, 382, 922, 3149, 1308, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Hello!, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "        \"MPhaniTarun is my full name.\")\n",
    "\n",
    "ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0140923d-466c-478d-808f-bbcee853c03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello!, do you like tea? <|endoftext|> In the sunlit terracesMPhaniTarun is my full name.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = tokenizer.decode(ids)\n",
    "strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcc9c9d5-9f36-4309-b736-98b3a4af6ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46066, 21189, 288, 1211, 66]\n"
     ]
    }
   ],
   "source": [
    "text = \" AWeredas idc\"\n",
    "ids = tokenizer.encode(text, allowed_special = {\"<|endoftext|>\"})\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "259c981e-027f-4c7e-8b3f-e78d70bffb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AW\n",
      " AW\n",
      "id\n",
      " id\n"
     ]
    }
   ],
   "source": [
    "string1 = tokenizer.decode([24853])\n",
    "string2 = tokenizer.decode([46066])\n",
    "string3 = tokenizer.decode([315])\n",
    "string4 = tokenizer.decode([1211])\n",
    "print(string1)   # With space in the start\n",
    "print(string2)   # without space\n",
    "print(string3)   # With space in the star\n",
    "print(string4)   # without space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851793d-3337-45aa-a97b-1149ed61f57f",
   "metadata": {},
   "source": [
    "<h2>Available BPEs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c9f776d-93b3-4618-84c3-f5e268e0177e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt2', 'r50k_base', 'p50k_base', 'p50k_edit', 'cl100k_base', 'o200k_base']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_encodings = tiktoken.list_encoding_names()\n",
    "available_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf4f06e-a947-4d9c-83b6-b9a477dad14f",
   "metadata": {},
   "source": [
    "<h2>Vocabulary</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c06b3212-1455-4bf4-8237-a3782f5e3728",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token ID: 0, Token: '!'\n",
      "Token ID: 1, Token: '\"'\n",
      "Token ID: 2, Token: '#'\n",
      "Token ID: 3, Token: '$'\n",
      "Token ID: 4, Token: '%'\n",
      "Token ID: 5, Token: '&'\n",
      "Token ID: 6, Token: \"'\"\n",
      "Token ID: 7, Token: '('\n",
      "Token ID: 8, Token: ')'\n",
      "Token ID: 9, Token: '*'\n",
      "Token ID: 10, Token: '+'\n",
      "Token ID: 11, Token: ','\n",
      "Token ID: 12, Token: '-'\n",
      "Token ID: 13, Token: '.'\n",
      "Token ID: 14, Token: '/'\n",
      "Token ID: 15, Token: '0'\n",
      "Token ID: 16, Token: '1'\n",
      "Token ID: 17, Token: '2'\n",
      "Token ID: 18, Token: '3'\n",
      "Token ID: 19, Token: '4'\n",
      "Token ID: 20, Token: '5'\n",
      "Token ID: 21, Token: '6'\n",
      "Token ID: 22, Token: '7'\n",
      "Token ID: 23, Token: '8'\n",
      "Token ID: 24, Token: '9'\n",
      "Token ID: 25, Token: ':'\n",
      "Token ID: 26, Token: ';'\n",
      "Token ID: 27, Token: '<'\n",
      "Token ID: 28, Token: '='\n",
      "Token ID: 29, Token: '>'\n",
      "Token ID: 30, Token: '?'\n",
      "Token ID: 31, Token: '@'\n",
      "Token ID: 32, Token: 'A'\n",
      "Token ID: 33, Token: 'B'\n",
      "Token ID: 34, Token: 'C'\n",
      "Token ID: 35, Token: 'D'\n",
      "Token ID: 36, Token: 'E'\n",
      "Token ID: 37, Token: 'F'\n",
      "Token ID: 38, Token: 'G'\n",
      "Token ID: 39, Token: 'H'\n",
      "Token ID: 40, Token: 'I'\n",
      "Token ID: 41, Token: 'J'\n",
      "Token ID: 42, Token: 'K'\n",
      "Token ID: 43, Token: 'L'\n",
      "Token ID: 44, Token: 'M'\n",
      "Token ID: 45, Token: 'N'\n",
      "Token ID: 46, Token: 'O'\n",
      "Token ID: 47, Token: 'P'\n",
      "Token ID: 48, Token: 'Q'\n",
      "Token ID: 49, Token: 'R'\n",
      "Token ID: 50, Token: 'S'\n",
      "Token ID: 51, Token: 'T'\n",
      "Token ID: 52, Token: 'U'\n",
      "Token ID: 53, Token: 'V'\n",
      "Token ID: 54, Token: 'W'\n",
      "Token ID: 55, Token: 'X'\n",
      "Token ID: 56, Token: 'Y'\n",
      "Token ID: 57, Token: 'Z'\n",
      "Token ID: 58, Token: '['\n",
      "Token ID: 59, Token: '\\\\'\n",
      "Token ID: 60, Token: ']'\n",
      "Token ID: 61, Token: '^'\n",
      "Token ID: 62, Token: '_'\n",
      "Token ID: 63, Token: '`'\n",
      "Token ID: 64, Token: 'a'\n",
      "Token ID: 65, Token: 'b'\n",
      "Token ID: 66, Token: 'c'\n",
      "Token ID: 67, Token: 'd'\n",
      "Token ID: 68, Token: 'e'\n",
      "Token ID: 69, Token: 'f'\n",
      "Token ID: 70, Token: 'g'\n",
      "Token ID: 71, Token: 'h'\n",
      "Token ID: 72, Token: 'i'\n",
      "Token ID: 73, Token: 'j'\n",
      "Token ID: 74, Token: 'k'\n",
      "Token ID: 75, Token: 'l'\n",
      "Token ID: 76, Token: 'm'\n",
      "Token ID: 77, Token: 'n'\n",
      "Token ID: 78, Token: 'o'\n",
      "Token ID: 79, Token: 'p'\n",
      "Token ID: 80, Token: 'q'\n",
      "Token ID: 81, Token: 'r'\n",
      "Token ID: 82, Token: 's'\n",
      "Token ID: 83, Token: 't'\n",
      "Token ID: 84, Token: 'u'\n",
      "Token ID: 85, Token: 'v'\n",
      "Token ID: 86, Token: 'w'\n",
      "Token ID: 87, Token: 'x'\n",
      "Token ID: 88, Token: 'y'\n",
      "Token ID: 89, Token: 'z'\n",
      "Token ID: 90, Token: '{'\n",
      "Token ID: 91, Token: '|'\n",
      "Token ID: 92, Token: '}'\n",
      "Token ID: 93, Token: '~'\n",
      "Token ID: 94, Token: '�'\n",
      "Token ID: 95, Token: '�'\n",
      "Token ID: 96, Token: '�'\n",
      "Token ID: 97, Token: '�'\n",
      "Token ID: 98, Token: '�'\n",
      "Token ID: 99, Token: '�'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Get the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "# Retrieve the entire vocabulary\n",
    "vocab_size = tokenizer.n_vocab\n",
    "vocab = {token_id: tokenizer.decode([token_id]) for token_id in range(vocab_size)}\n",
    "\n",
    "# Print a subset of the vocabulary\n",
    "for token_id, token in list(vocab.items())[:100]:  # Print the first 100 tokens\n",
    "    print(f\"Token ID: {token_id}, Token: {repr(token)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
