{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58920a95-f6d4-485e-ade7-a33513ada291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "272e669d-e6b8-4c9b-8132-0072f7b98d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4836\n"
     ]
    }
   ],
   "source": [
    "with open(\"Datasets/the-verdict.txt\", 'r', encoding = \"utf-8\") as f:\n",
    "    book_text = f.read()\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"o200k_base\") # GPT-4o \n",
    "enc_text = tokenizer.encode(book_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b927c885-1831-4788-80d8-503bdc1c0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing first 50 tokens\n",
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da287c8a-553d-49ca-b8ba-1f9f1c569cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x : [11166, 306, 261, 38350]\n",
      "y : [306, 261, 38350, 402]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(\"x :\", x)\n",
    "print(\"y :\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12187537-8581-4052-ac43-6f3e7d012875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11166] ----> 306\n",
      "[11166, 306] ----> 261\n",
      "[11166, 306, 261] ----> 38350\n",
      "[11166, 306, 261, 38350] ----> 402\n"
     ]
    }
   ],
   "source": [
    "# input target data pairs\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    input = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "\n",
    "    print(input, \"---->\", target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041423d2-eade-4578-b13e-5cbe60d608a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " himself ---->  in\n",
      " himself in ---->  a\n",
      " himself in a ---->  villa\n",
      " himself in a villa ---->  on\n"
     ]
    }
   ],
   "source": [
    "# Decoded input-target pairs\n",
    "\n",
    "for i in range(1, context_size+1):\n",
    "    input = enc_sample[:i]\n",
    "    target = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(input), \"---->\", tokenizer.decode([target]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720e2f4-fab2-4691-9eaa-102f6cd28052",
   "metadata": {},
   "source": [
    "<h2>Implementing via Data Loader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f89b4a-4c68-4ad3-af97-696da5ecbd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special = {\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunck = token_ids[i : i+max_length]\n",
    "            target_chunck = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunck))\n",
    "            self.target_ids.append(torch.tensor(target_chunck))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "774f91b8-0d58-4bc6-affa-2d41ebc17e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size = 4, max_length = 256,\n",
    "                         stride = 128, shuffle = True, drop_last = True,\n",
    "                         num_workers = 0):  # cant increase num_workers due to error, to increase move the GPTDatasetV1 class to .py file\n",
    "    tokenizer = tiktoken.get_encoding(\"o200k_base\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = shuffle,\n",
    "        drop_last = drop_last,\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e1f1383-b8e5-4d76-b2e8-53c50824cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Datasets/the-verdict.txt\", 'r', encoding = \"utf-8\") as f:\n",
    "    book_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe685136-fbd6-4a29-9b80-71ee5aad7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[    40, 148954,   3324,   4525]]), tensor([[148954,   3324,   4525,  10874]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    book_text, batch_size = 1, max_length = 4, stride = 1, shuffle = False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44599cd-a606-4c94-8c32-c5478f879979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[148954,   3324,   4525,  10874]]), tensor([[  3324,   4525,  10874, 165003]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1a8275d-b063-467e-b272-9ebcae4d2f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[    40, 148954,   3324,   4525],\n",
      "        [ 10874, 165003,  33750,   7542],\n",
      "        [   261,  12424,  59245,    375],\n",
      "        [  6460,    261,   1899,  19807],\n",
      "        [  4951,    375,    786,    480],\n",
      "        [   673,    860,   2212,  19005],\n",
      "        [   316,    668,    316,   9598],\n",
      "        [   484,     11,    306,    290]])\n",
      "Targets:\n",
      " tensor([[148954,   3324,   4525,  10874],\n",
      "        [165003,  33750,   7542,    261],\n",
      "        [ 12424,  59245,    375,   6460],\n",
      "        [   261,   1899,  19807,   4951],\n",
      "        [   375,    786,    480,    673],\n",
      "        [   860,   2212,  19005,    316],\n",
      "        [   668,    316,   9598,    484],\n",
      "        [    11,    306,    290,   4679]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(\n",
    "    book_text, batch_size = 8, max_length = 4, stride = 4, shuffle = False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"Targets:\\n\", targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
